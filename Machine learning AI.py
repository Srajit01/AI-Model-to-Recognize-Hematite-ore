# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17GjTyE1OXE71v9KFHKlEIcYnhMgl7AFN
"""

!nvidia-smi

!pip install tensorflow

# Install other required libraries
!pip install keras
!pip install efficientnet
!pip install numpy

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from google.colab import drive
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

data_dir = '/content/drive/MyDrive/Dataset'
import os
print(os.path.exists(data_dir))  # This should print True if the directory exists
train_dir = data_dir + '/train'
val_dir = data_dir + '/val'
test_dir = data_dir + '/test'

print(os.path.exists(train_dir))  # Should be True if the path is correct
print(os.path.exists(val_dir))    # Should be True if the path is correct
print(os.path.exists(test_dir))   # Should be True if the path is correct

# List contents to verify structure
print("Train contents:", os.listdir(train_dir))
print("Validation contents:", os.listdir(val_dir))
print("Test contents:", os.listdir(test_dir))

# Data preprocessing and augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Load data generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=5,
    class_mode='binary'
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=5,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=5,
    class_mode='binary'
)

# Transfer learning with EfficientNetB0
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

# Fine-tune last few layers
for layer in base_model.layers[:-5]:
    layer.trainable = True

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

epochs = 20
steps_per_epoch = len(train_generator)
val_steps = len(val_generator)

# from tensorflow.keras import layers, models

# # Assuming image dimensions and other hyperparameters are defined appropriately
# image_height = 128
# image_width = 128
# channels = 3  # Assuming RGB images
# num_epochs = 20  # You can adjust this based on your training results


# # Define the CNN model
# model = models.Sequential([
#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, channels)),
#     layers.MaxPooling2D((2, 2)),
#     layers.Conv2D(64, (3, 3), activation='relu'),
#     layers.MaxPooling2D((2, 2)),
#     layers.Conv2D(128, (3, 3), activation='relu'),
#     layers.MaxPooling2D((2, 2)),
#     layers.Flatten(),
#     layers.Dense(128, activation='relu'),
#     layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification
# ])

# # Compile the model
# model.compile(optimizer='adam',
#               loss='binary_crossentropy',
#               metrics=['accuracy'])

print("Train generator samples:", train_generator.samples)
print("Val generator samples:", val_generator.samples)
print("Test generator samples:", test_generator.samples)

# Check the number of batches per epoch
print("Steps per epoch:", steps_per_epoch)
print("Validation steps:", val_steps)

model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    validation_data=val_generator,
    validation_steps=val_steps
)
test_loss, test_acc = model.evaluate(test_generator)
print(f'Test accuracy: {test_acc * 100:.2f}%')

import os

# Save the compiled model
model.save('model.h5')

# Print the location where the model is saved
save_path = os.path.abspath('model.h5')
print(f'Model saved at: {save_path}')

from google.colab import files
from IPython.display import Image

# Upload an image file
uploaded = files.upload()

# Get the file name of the uploaded image
file_name = next(iter(uploaded))

# Display the uploaded image
display(Image(filename=file_name))

# # Function to load, preprocess, and predict a single image
# def predict_image(image_path):
#     img = load_and_preprocess_image(image_path)
#     prediction = model.predict(img)
#     if prediction[0][0] > 0.5:
#         print("hematite")
#         print(prediction[0][0])
#     else:
#         print("not-hematite")
#         print(prediction[0][0])

# Function to load and preprocess a single image (unchanged)
def load_and_preprocess_image(image_path):
    img = load_img(image_path, target_size=(224, 224))
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array /= 255.
    return img_array

def predict_image(image_path):
    img = load_and_preprocess_image(image_path=image_path)
    prediction = model.predict(img)
    if prediction[0][0] > 0.5:
        print("hematite")
        print(prediction[0][0])
    else:
        print("not-hematite")
        print(prediction[0][0])

# Usage example for manual testing
image_path = file_name  # Use the uploaded image file path
predict_image(image_path)

image_path = '/content/drive/MyDrive/Dataset/Hematite ore/Image_158.jpg'
# Display the uploaded image
display(Image(filename=image_path))
predict_image(image_path)

